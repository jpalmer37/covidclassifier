{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cognitive-soldier",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "/Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mimg\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "import imgaug as aug\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Input, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caring-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First thing we need to do is to extract the data from the zip folder.\n",
    "# If this has not already been done, then do it.\n",
    "if not (os.path.exists('TestImages') and os.path.exists(\"TrainImages\")):\n",
    "    shutil.unpack_archive('sacm2021.zip', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stylish-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to extract the training images and associate them to a label\n",
    "# Because the `train.csv` file has ids and classifications, I need to grab image ids from their file name.\n",
    "\n",
    "\n",
    "samples_path = Path('TrainImages/TrainImages/')\n",
    "samples_images = list(samples_path.glob('*.png'))\n",
    "\n",
    "samples_ids = list(\n",
    "    map(\n",
    "        lambda x: int(str(x).replace(\".png\",'').replace(str(samples_path), '').replace(\"/\", '')), \n",
    "        samples_images\n",
    "    )\n",
    ")\n",
    "\n",
    "y = pd.read_csv('train.csv')\n",
    "samples_frame = pd.DataFrame({'img': samples_images, \"id\": samples_ids}).merge(y)\n",
    "\n",
    "train, val = train_test_split(samples_frame, train_size=0.6, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intermediate-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will corrupt the images with the following:\n",
    "\n",
    "augmentations = [\n",
    "    iaa.Fliplr(), # horizontal flips\n",
    "    iaa.Affine(rotate=20), # roatation\n",
    "    iaa.Multiply((1.2, 1.5)),\n",
    "    iaa.GaussianBlur(sigma=(0, 3)),\n",
    "    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.005*255)),\n",
    "    iaa.Invert(1.0, min_value=0, max_value=255)\n",
    "    ]\n",
    "augmenting_squence = iaa.SomeOf((0,3), augmentations) #random brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "blessed-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(data, batch_size):\n",
    "    # Get total number of samples in the data\n",
    "    n = len(data)\n",
    "    steps = n//batch_size\n",
    "    \n",
    "    # Define two numpy arrays for containing batch data and labels\n",
    "    batch_data = np.zeros((batch_size, 224, 224, 3), dtype=np.float32)\n",
    "    batch_labels = np.zeros((batch_size,4), dtype=np.float32)\n",
    "\n",
    "    # Get a numpy array of all the indices of the input data\n",
    "    indices = np.arange(n)\n",
    "    \n",
    "    # Initialize a counter\n",
    "    i =0\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        # Get the next batch \n",
    "        count = 0\n",
    "        next_batch = indices[(i*batch_size):(i+1)*batch_size]\n",
    "        for j, idx in enumerate(next_batch):\n",
    "            img_name = data.iloc[idx]['img']\n",
    "            label = data.iloc[idx]['classification']\n",
    "            \n",
    "            # one hot encoding\n",
    "            encoded_label = to_categorical(label, num_classes=4)\n",
    "            # read the image and resize\n",
    "            img = cv2.imread(str(img_name))\n",
    "            img = cv2.resize(img, (224,224))\n",
    "            \n",
    "            # check if it's grayscale\n",
    "            if img.shape[2]==1:\n",
    "                img = np.dstack([img, img, img])\n",
    "            \n",
    "            # cv2 reads in BGR mode by default\n",
    "            orig_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            # normalize the image pixels\n",
    "            orig_img = img.astype(np.float32)\n",
    "            \n",
    "            batch_data[count] = augmenting_squence.augment_image(orig_img)/255\n",
    "            batch_labels[count] = encoded_label\n",
    "            \n",
    "            \n",
    "            \n",
    "            count+=1\n",
    "            \n",
    "            \n",
    "        i+=1\n",
    "        yield batch_data, batch_labels\n",
    "            \n",
    "        if i>=steps:\n",
    "            i=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "velvet-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation data\n",
    "\n",
    "valid_data = []\n",
    "valid_labels = []\n",
    "\n",
    "for row in val.to_dict(orient = 'records'):\n",
    "    img = cv2.imread(str(row['img']))\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32)/255.\n",
    "    label = to_categorical(row['classification'], num_classes=4)\n",
    "    valid_data.append(img)\n",
    "    valid_labels.append(label)\n",
    "    \n",
    "valid_data = np.array(valid_data)\n",
    "valid_labels = np.array(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "endless-customs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/demetri/opt/miniconda3/envs/hackshulich/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    input_img = Input(shape=(224,224,3), name='ImageInput')\n",
    "    x = Conv2D(64, (3,3), activation='relu')(input_img)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(32, (3,3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    \n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(4, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_img, outputs=x)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "opt = Adam(lr=0.0001, decay=1e-5)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "prompt-manhattan",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "35/35 [==============================] - 29s 818ms/step - loss: 1.3852 - acc: 0.3268 - val_loss: 1.3557 - val_acc: 0.3482\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 26s 741ms/step - loss: 1.3677 - acc: 0.3464 - val_loss: 1.3471 - val_acc: 0.3534\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 26s 738ms/step - loss: 1.3402 - acc: 0.3732 - val_loss: 1.3514 - val_acc: 0.3979\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 26s 754ms/step - loss: 1.3466 - acc: 0.3518 - val_loss: 1.3306 - val_acc: 0.3953\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 26s 752ms/step - loss: 1.3494 - acc: 0.3429 - val_loss: 1.3211 - val_acc: 0.3455\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 26s 744ms/step - loss: 1.3458 - acc: 0.3482 - val_loss: 1.3293 - val_acc: 0.3482\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 27s 759ms/step - loss: 1.3253 - acc: 0.3589 - val_loss: 1.3134 - val_acc: 0.3482\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 26s 746ms/step - loss: 1.3159 - acc: 0.3696 - val_loss: 1.2870 - val_acc: 0.3874\n",
      "Epoch 9/50\n",
      "35/35 [==============================] - 26s 753ms/step - loss: 1.3050 - acc: 0.3946 - val_loss: 1.2996 - val_acc: 0.3796\n",
      "Epoch 10/50\n",
      "35/35 [==============================] - 26s 744ms/step - loss: 1.3136 - acc: 0.3982 - val_loss: 1.2966 - val_acc: 0.4110\n",
      "Epoch 11/50\n",
      "35/35 [==============================] - 26s 738ms/step - loss: 1.2715 - acc: 0.4411 - val_loss: 1.2883 - val_acc: 0.3927\n",
      "Epoch 12/50\n",
      "35/35 [==============================] - 26s 751ms/step - loss: 1.2706 - acc: 0.4054 - val_loss: 1.2933 - val_acc: 0.3717\n",
      "Epoch 13/50\n",
      "35/35 [==============================] - 27s 768ms/step - loss: 1.2736 - acc: 0.3982 - val_loss: 1.2880 - val_acc: 0.3927\n",
      "Epoch 14/50\n",
      "35/35 [==============================] - 26s 749ms/step - loss: 1.2578 - acc: 0.4357 - val_loss: 1.2920 - val_acc: 0.4058\n",
      "Epoch 15/50\n",
      "35/35 [==============================] - 27s 761ms/step - loss: 1.2231 - acc: 0.4750 - val_loss: 1.2863 - val_acc: 0.4084\n",
      "Epoch 16/50\n",
      "35/35 [==============================] - 26s 740ms/step - loss: 1.2101 - acc: 0.4589 - val_loss: 1.3031 - val_acc: 0.3743\n",
      "Epoch 17/50\n",
      "35/35 [==============================] - 26s 740ms/step - loss: 1.2182 - acc: 0.4679 - val_loss: 1.3064 - val_acc: 0.3717\n",
      "Epoch 18/50\n",
      "35/35 [==============================] - 26s 740ms/step - loss: 1.2052 - acc: 0.4893 - val_loss: 1.3665 - val_acc: 0.3639\n",
      "Epoch 19/50\n",
      "35/35 [==============================] - 26s 754ms/step - loss: 1.1934 - acc: 0.4714 - val_loss: 1.3641 - val_acc: 0.3639\n",
      "Epoch 20/50\n",
      "35/35 [==============================] - 26s 749ms/step - loss: 1.1653 - acc: 0.5232 - val_loss: 1.3329 - val_acc: 0.4058\n",
      "Epoch 21/50\n",
      "35/35 [==============================] - 26s 743ms/step - loss: 1.2447 - acc: 0.4607 - val_loss: 1.3089 - val_acc: 0.3770\n",
      "Epoch 22/50\n",
      "35/35 [==============================] - 26s 738ms/step - loss: 1.1688 - acc: 0.5161 - val_loss: 1.3854 - val_acc: 0.3403\n",
      "Epoch 23/50\n",
      "35/35 [==============================] - 26s 751ms/step - loss: 1.1602 - acc: 0.5036 - val_loss: 1.3237 - val_acc: 0.4005\n",
      "Epoch 24/50\n",
      "35/35 [==============================] - 26s 757ms/step - loss: 1.1086 - acc: 0.5304 - val_loss: 1.3584 - val_acc: 0.3927\n",
      "Epoch 25/50\n",
      "35/35 [==============================] - 27s 759ms/step - loss: 1.1178 - acc: 0.5143 - val_loss: 1.3746 - val_acc: 0.3534\n",
      "Epoch 26/50\n",
      "35/35 [==============================] - 26s 741ms/step - loss: 1.0712 - acc: 0.5429 - val_loss: 1.4011 - val_acc: 0.3927\n",
      "Epoch 27/50\n",
      "35/35 [==============================] - 31s 881ms/step - loss: 1.1147 - acc: 0.5304 - val_loss: 1.3923 - val_acc: 0.3796\n",
      "Epoch 28/50\n",
      "35/35 [==============================] - 34s 959ms/step - loss: 1.1179 - acc: 0.5268 - val_loss: 1.3837 - val_acc: 0.3796\n",
      "Epoch 29/50\n",
      "35/35 [==============================] - 28s 789ms/step - loss: 1.0854 - acc: 0.5393 - val_loss: 1.3897 - val_acc: 0.3848\n",
      "Epoch 30/50\n",
      "35/35 [==============================] - 28s 804ms/step - loss: 1.1105 - acc: 0.5286 - val_loss: 1.4094 - val_acc: 0.3848\n",
      "Epoch 31/50\n",
      "35/35 [==============================] - 30s 844ms/step - loss: 1.0729 - acc: 0.5446 - val_loss: 1.3919 - val_acc: 0.3927\n",
      "Epoch 32/50\n",
      "35/35 [==============================] - 29s 830ms/step - loss: 1.0715 - acc: 0.5536 - val_loss: 1.5132 - val_acc: 0.4031\n",
      "Epoch 33/50\n",
      "35/35 [==============================] - 28s 796ms/step - loss: 1.0007 - acc: 0.5857 - val_loss: 1.4346 - val_acc: 0.4215\n",
      "Epoch 34/50\n",
      "35/35 [==============================] - 28s 793ms/step - loss: 1.0439 - acc: 0.5804 - val_loss: 1.4524 - val_acc: 0.4241\n",
      "Epoch 35/50\n",
      "35/35 [==============================] - 32s 918ms/step - loss: 1.0540 - acc: 0.5946 - val_loss: 1.4369 - val_acc: 0.3979\n",
      "Epoch 36/50\n",
      "35/35 [==============================] - 29s 817ms/step - loss: 1.0230 - acc: 0.5696 - val_loss: 1.5030 - val_acc: 0.3822\n",
      "Epoch 37/50\n",
      "35/35 [==============================] - 29s 819ms/step - loss: 1.0002 - acc: 0.5946 - val_loss: 1.5126 - val_acc: 0.3796\n",
      "Epoch 38/50\n",
      "35/35 [==============================] - 28s 802ms/step - loss: 0.9395 - acc: 0.6339 - val_loss: 1.5262 - val_acc: 0.3822\n",
      "Epoch 39/50\n",
      "35/35 [==============================] - 29s 826ms/step - loss: 0.9015 - acc: 0.6518 - val_loss: 1.5738 - val_acc: 0.3743\n",
      "Epoch 40/50\n",
      "35/35 [==============================] - 30s 860ms/step - loss: 0.9488 - acc: 0.6196 - val_loss: 1.4972 - val_acc: 0.3848\n",
      "Epoch 41/50\n",
      "35/35 [==============================] - 30s 851ms/step - loss: 1.0070 - acc: 0.5786 - val_loss: 1.4794 - val_acc: 0.4031\n",
      "Epoch 42/50\n",
      "35/35 [==============================] - 30s 852ms/step - loss: 0.9452 - acc: 0.6268 - val_loss: 1.5424 - val_acc: 0.3848\n",
      "Epoch 43/50\n",
      "35/35 [==============================] - 30s 847ms/step - loss: 0.9174 - acc: 0.6411 - val_loss: 1.5178 - val_acc: 0.3848\n",
      "Epoch 44/50\n",
      "35/35 [==============================] - 30s 859ms/step - loss: 0.9472 - acc: 0.6125 - val_loss: 1.5213 - val_acc: 0.4084\n",
      "Epoch 45/50\n",
      "35/35 [==============================] - 30s 848ms/step - loss: 0.8882 - acc: 0.6643 - val_loss: 1.4801 - val_acc: 0.4005\n",
      "Epoch 46/50\n",
      "35/35 [==============================] - 30s 858ms/step - loss: 0.8945 - acc: 0.6500 - val_loss: 1.5355 - val_acc: 0.3953\n",
      "Epoch 47/50\n",
      "35/35 [==============================] - 29s 842ms/step - loss: 0.9590 - acc: 0.6411 - val_loss: 1.4831 - val_acc: 0.3874\n",
      "Epoch 48/50\n",
      "35/35 [==============================] - 30s 861ms/step - loss: 0.9131 - acc: 0.6446 - val_loss: 1.6315 - val_acc: 0.3691\n",
      "Epoch 49/50\n",
      "35/35 [==============================] - 30s 846ms/step - loss: 0.9228 - acc: 0.6393 - val_loss: 1.6016 - val_acc: 0.3822\n",
      "Epoch 50/50\n",
      "35/35 [==============================] - 30s 865ms/step - loss: 0.8563 - acc: 0.6661 - val_loss: 1.5299 - val_acc: 0.4058\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "nb_epochs = 50\n",
    "\n",
    "# Get a train data generator\n",
    "train_data_gen = data_gen(data=train, batch_size=batch_size)\n",
    "\n",
    "# Define the number of training steps\n",
    "nb_train_steps = train.shape[0]//batch_size\n",
    "\n",
    "\n",
    "# # Fit the model\n",
    "history = model.fit_generator(train_data_gen, \n",
    "                              epochs=nb_epochs, \n",
    "                              steps_per_epoch=nb_train_steps,\n",
    "                              validation_data=(valid_data, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "informed-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_samples_path = Path('TestImages/TestImages/')\n",
    "test_samples_images = list(test_samples_path.glob('*.png'))\n",
    "\n",
    "test_samples_ids = list(\n",
    "    map(\n",
    "        lambda x: str(x).replace(\".png\",'').replace(str(test_samples_path), '').replace(\"/\", ''), \n",
    "        test_samples_images\n",
    "    )\n",
    ")\n",
    "\n",
    "test_samples_frame = pd.DataFrame({'img': test_samples_images, \"id\": test_samples_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "alpine-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = []\n",
    "for img in test_samples_images:\n",
    "    img = cv2.imread(str(img))\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32)/255.\n",
    "    \n",
    "    pred = model.predict(img.reshape(1, 224,224,3)).argmax()\n",
    "    \n",
    "    lbl.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "terminal-spanking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(img.reshape(1, 224,224,3)).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "following-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_frame['classification'] = lbl\n",
    "test_samples_frame.loc[:, ['id','classification']].to_csv('sub.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-aaron",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
